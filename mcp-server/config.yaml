# EDA Server Configuration
# All thresholds and parameters centralized for easy tuning

# Orchestrator Configuration
orchestrator:
  max_concurrent_workflows: 1
  retry:
    max_retries: 3
    backoff_base_s: 30
    backoff_max_s: 300
  scheduling:
    sla_task_complete_s: 600      # 10 minutes
    sla_workflow_complete_s: 3600 # 1 hour
  workload_estimate:
    tasks_per_hour: 30
    avg_task_duration_s: 240      # 4 minutes

# Master Orchestrator Configuration
master_orchestrator:
  # LLM Translation Settings
  llm:
    model_version: "claude-3-sonnet-20240229"
    max_input_length: 10000
    llm_max_tokens: 4000
    llm_max_retries: 3
    temperature: 0.0
    rail_schema_path: "orchestrator/rail_schema.xml"
    system_prompt: |
      You are a workflow orchestrator that converts natural language requests into structured DSL YAML.
      Output ONLY valid JSON that follows the expected schema with tasks, agents, and actions.
      
  # Rule-based Translation Settings  
  rules:
    rule_mappings:
      "load data":
        id: "load_data_task"
        agent: "eda_agent"
        action: "load_data"
        params: {"file": "data.csv"}
      "analyze data":
        id: "analyze_task"
        agent: "eda_agent" 
        action: "analyze_data"
        params: {}
      "create visualization":
        id: "viz_task"
        agent: "eda_agent"
        action: "create_visualization"
        params: {"type": "histogram"}
      "train model":
        id: "train_task"
        agent: "ml_agent"
        action: "train_model"
        params: {"algorithm": "random_forest"}
      "evaluate model":
        id: "eval_task"
        agent: "ml_agent"
        action: "evaluate_model"
        params: {}
  
  # Fallback Settings
  enable_human_fallback: true
  min_confidence_threshold: 0.7
  
  # Infrastructure Settings
  mongo_url: "mongodb://localhost:27017"
  db_name: "deepline"
  
  # Decision Engine Settings
  decision:
    gpu_agents: ["ml_agent", "deep_learning_agent"]
    cpu_agents: ["eda_agent", "data_agent", "analysis_agent"]
    max_task_count: 100
    blocked_actions: []
    priority_agents: ["critical_analysis_agent"]
    maintenance_windows: []
    resource_limits:
      max_concurrent_gpu_tasks: 2
      max_concurrent_cpu_tasks: 10
      max_memory_per_task_gb: 16
    model_rules: {}
  
  # Telemetry & Tracing Settings
  telemetry:
    enabled: true
    service_name: "master-orchestrator"
    service_version: "1.0.0"
    otlp_endpoint: "http://localhost:4318/v1/traces"  # Optional: configure for your OTLP collector
  kafka_bootstrap_servers: "localhost:9092"
  task_requests_topic: "task.requests"
  task_events_topic: "task.events"
  
  # Rate Limiting
  rate_limits:
    requests_per_minute: 60
    requests_per_hour: 1000
    burst_requests: 10
  
  # SLA Monitoring
  sla:
    check_interval_seconds: 30
    task_timeout_seconds: 600     # 10 minutes
    workflow_timeout_seconds: 3600 # 1 hour
  
  # Cache Settings
  cache:
    redis_url: "redis://localhost:6379"
    namespace: "master_orchestrator"
    default_ttl: 3600

# Missing Data Analysis
missing_data:
  column_drop_threshold: 0.50  # 50% - drop columns with >50% missing data
  row_drop_threshold: 0.50     # 50% - drop rows with >50% missing values
  systematic_correlation_threshold: 0.70  # High correlation for systematic missingness
  imputation:
    skew_threshold: 1.0        # Use median if |skew| > 1, else mean
    categorical_mode_threshold: 0.20  # Only impute if missing < 20%

# Outlier Detection
outlier_detection:
  iqr_factor: 1.5              # Standard IQR multiplier
  contamination_default: 0.05  # 5% expected outliers for model-based methods
  mahalanobis_confidence: 0.975  # Chi-square confidence level
  max_columns_visualized: 10   # Limit columns for performance
  sample_size_limit: 10000     # Sample size for large datasets

# Schema Inference
schema_inference:
  id_uniqueness_threshold: 0.90  # 90% uniqueness for ID detection
  datetime_success_rate: 0.80   # 80% success rate for datetime inference
  precision_sample_size: 100    # Sample size for precision/scale inference
  max_sample_values: 5          # Max sample values to show

# Feature Transformation
feature_transformation:
  rare_category_threshold: 0.005  # 0.5% threshold for rare categories
  vif_severe_threshold: 10.0      # VIF > 10 indicates severe multicollinearity
  vif_moderate_threshold: 5.0     # VIF > 5 indicates moderate multicollinearity
  boxcox_epsilon: 1e-6            # Small epsilon for Box-Cox shifting
  skew_improvement_threshold: 0.5 # Minimum skewness improvement for transformation
  binning_n_bins: 5               # Default number of bins for quantile binning
  supervised_binning_min_samples: 10  # Minimum samples per bin

# Visualization
visualization:
  correlation_sample_size: 10000  # Sample size for correlation matrices
  max_points_scatter: 5000        # Max points for scatter plots
  figure_dpi: 150                 # DPI for saved figures
  correlation_label_threshold: 0.5 # Show labels for correlations > 0.5

# Performance
performance:
  memory_warning_threshold: 1000  # MB - warn if dataset > 1GB
  max_rows_processed: 100000      # Max rows for intensive operations
  chunk_size: 10000               # Chunk size for large dataset processing

# Human Checkpoints
checkpoints:
  require_approval: true          # Whether to require human approval
  approval_timeout: 300           # Seconds to wait for approval
  auto_approve_small_changes: true  # Auto-approve minor transformations 